\documentclass{school-22.211-notes}
\date{April 4, 2012}

\begin{document}
\maketitle

\lecture{Two-Group Diffusion: Numerical Solutions}
Reference: Stacy's Section 3.10. 

\topic{Simple Numerical Methods to Solve Diffusion Equations}
\subtopic{Derivation of Neutron Balance Equation}
Recall the two-group neutron balance equations:
\begin{itemize}
\item Total fission source is: 
  \eqn{S_f(r) = \nu \Sigma_{f1} (r) \phi_1 (r) + \nu \Sigma_{f2} (r) \phi_2(r) }
  $\chi_1 = 1, \chi_2 = 0$ which implies that fission source in the thermal group is zero.
\item Scattering source: we define effective down-scattering, so up-scattering is zero $\Sigma_{21}(r) = 0$. 
\item Two-group diffusion equations: 
  \begin{align}
    \left\{ \begin{array}{c}
      - D_1 \laplacian \phi_1 + (\Sigma_{a1} + \Sigma_{s12}) \phi_1 = \nu \Sigma_{f1} \phi_1 + \nu \Sigma_{f2} \phi_2 + S_1  \\
      - D_2 \laplacian \phi_2 + \Sigma_{a2} \phi_2 = \Sigma_{s12} \phi_1 + S_2
    \end{array} \right. 
  \end{align}
\end{itemize}

\subtopic{Derivation of Interface Flux}
Then we construct a finite spatial mesh in which cross sections are constant; integrate the neutron diffusion equations over each mesh cell $\Delta^n$,
\eqn{-\int \ddx D_1^n \ddx \phi_1^n \dx + \Sigma_{r1}^n  \phi_1^n \Delta^n = \nu \Sigma_{f1}^n \phi_1^n \Delta^n + \nu \Sigma_{f2}^n \phi_2^n \Delta^n + S_1 \Delta^n} 
\eqn{-\int \ddx D_2^n \ddx \phi_2^n  \dx + \Sigma_{a2}^n \phi_2^n \Delta^n = \Sigma_{s12}^n \phi_1^n \Delta^n + S_2 \Delta^n}
We evaluate the integral term\footnote{Lectuer says use the divergence theorem $\int \divergence \vec{F} \dV = \int_S \vec{F} \cdot \vec{n} \dS$, I think this is just manipulating integral and $\int_{x_1}^{x_2} \ddx A \dx \to \left. A \right|_{x_2} - \left. A \right|_{x_1}$}, 
\eqn{\int \ddx D_1^n \ddx \phi_1^n \dx  = \left. D_1^n \ddx \phi_1^n \right|_{n^+} - \left. D_1^n \ddx \phi_1^n \right|_{n^-} }
Then we have,
\eqn{\left. D_1^n \ddx \phi_1^n \right|_{n^-} - \left. D_1^n \ddx \phi_1^n \right|_{n^+} + \Sigma_{r1}^n  \phi_1^n \Delta^n = \nu \Sigma_{f1}^n \phi_1^n \Delta^n + \nu \Sigma_{f2}^n \phi_2^n \Delta^n + S_1 \Delta^n} 
\eqn{\left. D_2^n \ddx \phi_2^n \right|_{n^-} - \left. D_2^n \ddx \phi_2^n \right|_{n^+} + \Sigma_{a2}^n \phi_2^n \Delta^n = \Sigma_{s12}^n \phi_1^n \Delta^n + S_2 \Delta^n}
We notice that the $-D \ddx \phi$ terms are just current as in Figure~\ref{diagram-finite-difference}, 
\begin{figure}[ht]
  \centering
  \includegraphics[width=2.5in]{images/dfs/diagram-finite-difference.png}
  \caption{Diagram Illustrating Two Neighboring Cells in Finite Difference Scheme} \label{diagram-finite-difference}
\end{figure}
\eqn{J_n^+ = - \left. D_g^n \ddx \phi_g^n \right|_{n+} = -D_g^n \frac{\phi_g^S - \phi_g^n}{\Delta/2} }
\eqn{J_{n+1}^- = - \left. D_g^{n+1} \ddx \phi_g^{n+1} \right|_{n-} = - D_g^{n+1} \frac{\phi_g^{n+1} - \phi_g^s}{\Delta/2} }
By continuity of the net current, we can set the above two equations to equal to each other, 
\eqn{  -D_g^n \frac{\phi_g^S - \phi_g^n}{\Delta/2} =  - D_g^{n+1} \frac{\phi_g^{n+1} - \phi_g^s}{\Delta/2} }
From where we can solve for the interface flux, 
\eqn{ \phi_g^s = \frac{D_g^{n+1} \phi_g^{n+1} + D_g^n \phi_g^n}{D_g^n + D_g^{n+1}}} 
Then we can get th net current in terms of mesh fluxes: 
\eqn{ J_n^+ = - \frac{2D_g^n}{\Delta} \left[ \frac{ D^{n+1} \phi^{n+1} + D^n \phi^n}{D^n + D^{n+1}} - \phi^n \right] = - \overbrace{\frac{2 D^n D^{n+1} }{\Delta (D^n + D^{n+1})}}^{\to \hat{D}^{n,n+1}} (\phi^{n+1} - \phi^{n}) }
That is, 
\eqn{ J_n^+ &= - \hat{D}^{n,n+1} (\phi^{n+1} - \phi^n)  & J_n^- &= - \hat{D}^{n-1, n} (\phi^n - \phi^{n-1} ), & \hat{D}^{n,n+1} &= \frac{2 D^n D^{n+1}}{\Delta (D^n + D^{n+1})}  \label{net-current}} 


\subtopic{Derivation of Finite Difference Equations}
Plug Eq.~\ref{net-current} back into diffusion equations, we get, 
\eqn{ \hat{D}_1^{n-1,n} (\phi_1^n - \phi_1^{n-1})  - \hat{D}_1^{n,n+1} (\phi_1^{n+1} - \phi_1^n) + \Sigma_{r1}^n \phi_1^n \Delta^n &= \mu \Sigma_{f1}^n \phi_1^n \Delta^n + \mu \Sigma_{f2}^n \phi_2^n \Delta^n + S_1 \Delta^n} 
\eqn{ \hat{D}_2^{n-1,n} (\phi_2^n - \phi_2^{n-1})  - \hat{D}_2^{n,n+1} (\phi_2^{n+1} - \phi_2^n) + \Sigma_{a2}^n \phi_2^n \Delta^n &= \Sigma_{s12}^n \phi_1^n \Delta^n + S_2 \Delta^n} 
Rearranging terms by fluxes, we get, 
\eqn{ - \hat{D}_1^{n-1,n} \phi_1^{n-1} - \hat{D}_1^{n,n+1} \phi_1^{n+1} + [ \Sigma_{r1}^n \Delta^n + \hat{D}_1^{n-1, n} + \hat{D}_1^{n,n+1} ] \phi_1^n = \nu \Sigma_{f1}^n \phi_1^n \Delta^n + \nu \Sigma_{f2}^n \phi_2^n \Delta^n + S_1^n \Delta^n} 
\eqn{ - \hat{D}_2^{n-1,n} \phi_2^{n-1} - \hat{D}_2^{n,n+1} \phi_2^{n+1} + [\Sigma_{a2}^n \Delta^n + \hat{D}_2^{n-1.n} + \hat{D}_2^{n,n+1} ] \phi_2^n = \Sigma_{s12}^n \phi_1^n \Delta^n + S_2^n \Delta^n }

\subtopic{Derivation of Boundary Condition}
Interior of the geometry, the boundary conditions are implied. The exterior boundaries have to be specified. There are two types of boundary conditions:
\begin{enumerate}
\item Zero Flux BC: 
\eqn{ J_g^N = -D_g^N \frac{ -\phi_g^N - \phi_g^N}{\Delta^N}  = \frac{2 D_g^N}{\Delta^N} \phi_g^N }
\item Zero Incoming Current BC:
\eqn{ J^- = \frac{1}{4} \phi - \frac{1}{2} J_n = 0 }
which implies,
\begin{align}
  J_g^N &= \frac{\phi_g^s}{2} = -D_g^N \frac{\phi_g^s - \phi_g^N}{\Delta/2} \\
  &= - \frac{2 D_g^N}{\Delta^N} \frac{\frac{2D_g^N}{\Delta^N} - 1}{\frac{1}{2} + \frac{2D_g^N}{\Delta}} \phi_g^N \\
  &= \boxed{ \frac{2D_g^N}{\Delta^N} \left[ \frac{1}{1 + \frac{4 D_g^N}{\Delta^N}} \right] \phi_g^N } \label{JgN}
\end{align}
In Eqn.~\ref{JgN}, if $D \to 0$, we get zero flux boundary condition; if $D \to \infty$, we get zero current boundary condition. 
\end{enumerate}

\clearpage
\topic{Matrix Representation of 1D Slab Diffusion Equations}
\subtopic{Construct Matrix}
If we use $L$ for the $\phi^{n-1}$ term, $U$ for the $\phi^{n+1}$ term, $D$ for the $\phi^n$ term, $T$ for the transport term, we can express the finite difference equation in matrix form as: 
\begin{align}
[L_1 + D_1 + U_1] [\phi_1] &= [M_1] [\phi_1] + [M_2][\phi_2] + [S_1] \\
[L_2 + D_2 + U_2] [\phi_2] &= [T_2] [\phi_1] + [S_2] 
\end{align}
We define a vector of group fluxes, 
\begin{align}
\left[ \begin{array}{cc} 
[L_1 + D_1 + U_1] & [0] \\
-[T_2] & [L_2 + D_2 + U_2] \\
\end{array} \right] 
\left[ \begin{array}{c}
\phi_1 \\ \phi_2 \\ \end{array} \right] 
= \left[ {\begin{array}{cc} \left[M_1\right] & \left[M_2\right] \\ \left[0\right] & \left[0\right] \end{array}} \right] 
\left[ \begin{array}{c}
\phi_1 \\ \phi_2 \\ \end{array} \right] 
+ 
\left[ \begin{array}{c} 
S_1 \\ S_2 \\ \end{array} \right] 
\end{align}
Whose compressed form is, 
\eqn{ [A] [\phi] = [M] [\phi] + [S] }
The two matrix are plotted in Figure~\ref{matrix-form}. Notice there are missing points in A; explaination: Block matrix: there is no coupling of group 1 left flux to the group 2 right flux. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=6in]{images/dfs/matrix-form.png}
  \caption{Destruction and Production Matrix} \label{matrix-form}
\end{figure}

\clearpage
\subtopic{Method 1: Sequential Source/Fixed-Source Problem}
\begin{align}
[A] [\phi] &= [M] [\phi] + [S] \\
[A - M ] [\phi] &= [S] \\
[\phi] &= [A - M]^{-1} [S]
\end{align}
In this method, we guess $\keff$, and given a source we can solve for the flux. We can either use Matlab to solve the $Ax = b$ system, or we use Gaussian elimination/forward elimination backward substitution by hand. The basic idea is to subtract $a_{i,i-1}/a_{i-1,i-1}$ times the $(i-1)$th equation from the $i$th equation to eliminate the $a_{i,i-1}$ element in the $i$th equation. The modified $i$th equation is then divided by $a_{i,i}$. This process is repeated successively for $i=1$ through $i = I-1$. 

We never solve real questions this way, because the flux approaches the right shape as we increase the $\keff$ to approach the correct $\keff$, but once we pass the correct $\keff$, the flux can be entirely inverted to become negative (now that I think about it, this is fine because the flux solved can be arbitrarily scaled). If we have a super-critical problem, then the only way to get criticality with an additional source is through negative flux. A reactor is like an amplifier; the closer it is to criticality the more amplification it is; when critical, the amplification is infinity. 


\subtopic{Method 2: Direct Matlab Eigenvalue Solver}
We can solve for $\keff$ directly from: 
\begin{align}
[A] [\phi] &= \frac{1}{\keff} [M] [\phi] \\
[A]^{-1} [M] [\phi] &= \keff [\phi]
\end{align}
Notice that the eigenvectors are arbitrarily normalized; hence it is fine if the flux is negative. 


\subtopic{Method 3: Power Iterations}
The equation we are trying to solve is,
\eqn{ [A] [\phi]^{n+1} &= [M] [\phi]^n }
An initial guess of the flux $\phi_i^{(0)}$ at each point and of the eigenvalue $\lambda^{(0)}$ is made and an initial fission source at each point is constructed $S_i^{(0)} = M \phi_i^{(0)}$. The Gaussian elimination is performed to determine $\phi_i^{(1)} = A^{-1} S_i^{(0)}$. Then we update the source term as well based on the flux. A new estimate of the eigenvalue is made from: 
\eqn{ \keff = \frac{S_i^{(1)}}{S_i^{(0)}} }
The process is continued until the eigenvalues obtained on two successive iterates differ by less than a threshold level. The rate our fission source converges depends on material, geometry etc. See next section for \hi{dominance ratio}. 
\begin{figure}
  \centering
  \includegraphics[width=4in]{images/dfs/power-iteration-convergence.png}
  \caption{Power Iteration Convergence Rate (left) and Dominance Ratio (right)}
\end{figure}

\subtopic{Method 3+: Power Iterations with Gauss-Jacobi Numerical Inversion of Flux Matrix}
In a steady-state problem, it does not matter whether we fully converge our flux iteration. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=6in]{images/dfs/power-iteration-Gauss-Jacobi.png}
  \caption{Power Iteration With Gauss-Jacobi Numerical Inversion of Matrix}
\end{figure}

\clearpage
\topic{Matrix Representation of Higher Dimensions Diffusion Equations}
\begin{enumerate}
\item 1D matrix can be solved without iterations; this is not the case for higher dimenisions: we have to use iterative methods for higher dimensions. 
\item The matrix representation for higher dimensions looks the same as in 1D, except with one additional pair of off-diagonal term for each additional dimension as in Figure~\ref{matrix-shape}
\item The size of the matrix increases as the dimensions increases. For instance, if we have 10 meshes per dimension, then 1D matrix is 10x10, 2D matrix is 100x100, 3D matrix is 1000x1000.
\end{enumerate}
\begin{figure}[ht]
  \centering
  \includegraphics[width=5in]{images/dfs/matrix_multidimension.png}
  \caption{Destruction and Production Matrix Representation} \label{matrix-shape}
\end{figure}

\clearpage
\topic{Dominance Ratio}
Given an eigenvalue problem, if we specify the solved eigenvalues to be:
\eqn{ |\lambda_1| > |\lambda_2| \ge |\lambda_3| \ge \cdots }
then $|\lambda_1|$ is the spectral radius of the iteration matrix, and every mode has a dominance ratio,
\eqn{ \dr_n = \frac{\lambda_n}{\lambda_1} }
and power iteration kills of the lowest dominance ratio modes. The last remaining mode is the fundamental mode $dr = \frac{\lambda_2}{\lambda_1}$. If $|\lambda_1| \ge 1$, the ietration scheme is unstable and it would not converge\footnote{See Lewis' Computational Methods of Neutron Transport Appendixe C for more details}. Convergence of the power method is slow when $\dr$ is close to unity; in fact in most numerical methods, convergence rate = 1 - dominance ratio. Knowing the dominance ratio, we can estimate the number of iterations needed for convergence. 

Dominance ratio measures the spatial decoupling. It depends on: 
\begin{enumerate}
\item Symmetric mode. If the initial guess is symmetric, the solution is symmetric, and the method used is symmetric, then only the symmetric mode shows up in the dominance ratio, and the asymmetric mode is hidden. As shown in Figure~\ref{dominance-symmetric}, the symmetric guess may not display the highest $\dr$ convergence (0.99 in this case), instead it display a symmetric mode (0.975 in this case). Whereas a random guess would excited all modes and show all the $\dr$ convergence (though the lower modes die too fast to be observed; we can only see the last two dominance ratio, 0.97 and 0.99), and the dominant mode has an asymmetric shape.  
\begin{figure}[ht]
  \centering
  \includegraphics[width=6in]{images/dfs/dominance_ratio.png}
  \caption{Dominance Ratio With An Symmetric Guess (left) or Random Guess (right)}
  \label{dominance-symmetric}
\end{figure}

\item Core size: As seen in Figure~\ref{dominance-size}, as core size increases, dominance ratio increases, and it takes longer to converge. 
\begin{figure}
  \centering
  \includegraphics[width=7in]{images/dfs/dominance_ratio_size.png}
  \caption{Dominance Ratio With Different Core Sizes}
  \label{dominance-size}
\end{figure}

\item Decoupling of radial zones from inserting control rod, having asymmetric core loadings, or having xenon distributions. 

\item Decoupling of axial zones form partially inserting control rods, axial fuel enrichment zoning, and axial burnable absorber loading. 
\end{enumerate}


\clearpage
\topic{Summary}
Remember for real 3D problem,
\begin{itemize}
\item Matrix inversion if of order $N^3$, so in real applications no matrix inversion;
\item Finding all eigenvalues is at least order $N^2$;
\item Iterative inversion must be order $N$ to be practical for large problems;
\item Multi-level iteration is a practical necessity. 
\item Dominance ratio measures spatial decoupling-ness.
\end{itemize}


\end{document}
